{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a23ea7a-9121-411b-bc74-99bec40021ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. Explain the assumptions required to use ANOVA and provide examples of violations that could impact\n",
    "the validity of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbce7c14-7210-49ad-8499-eb1d140eefa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Assumptions of ANOVA\n",
    "Analysis of Variance (ANOVA) is a statistical technique used to compare the means of multiple groups.\n",
    "\n",
    "To ensure the validity of the results, several assumptions must be met:\n",
    "\n",
    "Normality: The dependent variable should be normally distributed within each group. \n",
    "This means that the data points in each group should follow a bell-shaped curve.\n",
    "Homogeneity of Variance: The variance of the dependent variable should be equal across all groups. \n",
    "This assumption is also known as homoscedasticity.\n",
    "Independence: The observations within each group should be independent.\n",
    "This means that the value of one observation should not be influenced by the value of another observation.\n",
    "\n",
    "Violations and Their Impact\n",
    "If any of these assumptions are violated, it can affect the validity of the ANOVA results.\n",
    "\n",
    "Here are some examples of violations and their potential consequences:\n",
    "\n",
    "Violation of Normality\n",
    "Skewness or Kurtosis: If the data distribution is skewed or has excessive kurtosis, it can violate the normality assumption.\n",
    "This can lead to inaccurate p-values and biased results.\n",
    "Example: A skewed distribution might occur if there are a few extreme outliers in the data.\n",
    "\n",
    "Violation of Homogeneity of Variance\n",
    "Heteroscedasticity: If the variance of the dependent variable is unequal across groups, \n",
    "it can violate the homogeneity of variance assumption. This can affect the accuracy of the F-test and p-values.\n",
    "Example: A violation of homogeneity of variance might occur if one group has a much larger spread of data points than the others.\n",
    "\n",
    "Violation of Independence\n",
    "Dependent Observations: If the observations within a group are not independent, it can violate the independence assumption.\n",
    "This can lead to inflated or deflated p-values.\n",
    "Example: A violation of independence might occur if the same individuals are measured multiple times or if data points are related\n",
    "to each other in a systematic way.\n",
    "\n",
    "To address these violations, you may need to:\n",
    "\n",
    "Transform the data: If the data is skewed, you might try transforming it using a logarithmic or square root transformation.\n",
    "Use a non-parametric test: If the normality assumption is severely violated, you could consider using a non-parametric alternative \n",
    "to ANOVA, such as the Kruskal-Wallis test.\n",
    "Use a robust ANOVA method: There are robust ANOVA methods that are less sensitive to violations of assumptions.\n",
    "Check for outliers: If there are extreme outliers, you might consider removing them or using a robust statistical method.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecc34e6-317e-47f5-a78f-94f02e51cad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. What are the three types of ANOVA, and in what situations would each be used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126e8377-d841-4b08-84c9-e34ecbbaa725",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Three Types of ANOVA\n",
    "ANOVA (Analysis of Variance) is a statistical technique used to compare the means of multiple groups. \n",
    "\n",
    "There are three main types of ANOVA:\n",
    "\n",
    "One-Way ANOVA:\n",
    "\n",
    "Purpose: Compares the means of three or more independent groups.\n",
    "Situation: Used when you have a single independent variable (factor) with multiple levels and want to determine if there are significant differences in the means of the dependent variable across those levels.\n",
    "Example: Comparing the mean test scores of students from three different schools.\n",
    "\n",
    "Two-Way ANOVA:\n",
    "\n",
    "Purpose: Compares the means of multiple groups based on two independent variables (factors).\n",
    "Situation: Used when you have two independent variables and want to determine if there are significant differences in the means of the dependent variable due to each factor, as well as any interaction between the factors.\n",
    "Example: Comparing the mean plant growth rates based on two factors: type of fertilizer and amount of sunlight.\n",
    "\n",
    "Repeated Measures ANOVA:\n",
    "\n",
    "Purpose: Compares the means of the same group of participants measured multiple times.\n",
    "Situation: Used when you have a single group of participants and want to determine if there are significant differences in the means of the dependent variable over time or across different conditions.\n",
    "Example: Comparing the mean blood pressure of the same individuals before, during, and after a stress-inducing task.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7daa03d-c365-4b07-9312-5e4788e3bcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3. What is the partitioning of variance in ANOVA, and why is it important to understand this concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244dda42-bd7b-49e9-9b91-19cb774645e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Partitioning of Variance in ANOVA\n",
    "Partitioning of variance in ANOVA is the process of dividing the total variability in the dependent variable into two components:\n",
    "\n",
    "Between-group variance: The variation in the dependent variable that is due to differences between the means of the different groups.\n",
    "Within-group variance: The variation in the dependent variable that is due to differences within each group.\n",
    "\n",
    "Why is understanding partitioning of variance important?\n",
    "Hypothesis Testing: The F-test statistic in ANOVA is calculated by comparing the between-group variance to the within-group variance. If the between-group variance is significantly larger than the within-group variance, it suggests that the means of the groups are significantly different.\n",
    "Effect Size: The proportion of the total variance in the dependent variable that is explained by the independent variable(s) is a measure of effect size. This can be calculated by dividing the between-group variance by the total variance.\n",
    "Understanding Variation: Partitioning of variance helps you to understand the sources of variation in your data. This can be useful for identifying factors that are important in explaining the dependent variable.\n",
    "Example\n",
    "Imagine a study comparing the test scores of students from three different schools.\n",
    "\n",
    "The total variance in test scores can be partitioned into:\n",
    "\n",
    "Between-group variance: The differences in average test scores between the three schools.\n",
    "Within-group variance: The individual differences in test scores within each school.\n",
    "If the between-group variance is significantly larger than the within-group variance,\n",
    "it suggests that the schools have significantly different average test scores. \n",
    "This information can be used to identify factors that contribute to the differences in test scores between the schools.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdba186d-3115-466f-86f5-52d8e69d41a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. How would you calculate the total sum of squares (SST), explained sum of squares (SSE), and residual\n",
    "sum of squares (SSR) in a one-way ANOVA using Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103253da-5b07-4f5c-97c0-8e9f689bba38",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import numpy as np\n",
    "\n",
    "def calculate_anova_sums(data):\n",
    "    \"\"\"Calculates SST, SSE, and SSR for a one-way ANOVA.\n",
    "\n",
    "    Args:\n",
    "        data: A 2D NumPy array where each row represents a group and each column represents a data point.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing SST, SSE, and SSR.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the overall mean\n",
    "    grand_mean = np.mean(data)\n",
    "\n",
    "    # Calculate the sum of squares total (SST)\n",
    "    sst = np.sum((data - grand_mean)**2)\n",
    "\n",
    "    # Calculate the sum of squares between groups (SSE)\n",
    "    n_groups, n_obs = data.shape\n",
    "    group_means = np.mean(data, axis=1)\n",
    "    sse = np.sum(n_obs * (group_means - grand_mean)**2)\n",
    "\n",
    "    # Calculate the sum of squares within groups (SSR)\n",
    "    ssr = sst - sse\n",
    "\n",
    "    return sst, sse, ssr\n",
    "\n",
    "# Example usage\n",
    "data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "sst, sse, ssr = calculate_anova_sums(data)\n",
    "\n",
    "print(\"SST:\", sst)\n",
    "print(\"SSE:\", sse)\n",
    "print(\"SSR:\", ssr)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac951d8e-c399-4775-9a66-536a7568e183",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5. In a two-way ANOVA, how would you calculate the main effects and interaction effects using Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f20eb6a-5690-466f-b24c-9178afc7d75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "def calculate_two_way_anova(data, factor1, factor2):\n",
    "    \"\"\"Calculates main effects and interaction effects for a two-way ANOVA.\n",
    "\n",
    "    Args:\n",
    "        data: A 2D NumPy array where each row represents a combination of factor levels and each column represents a data point.\n",
    "        factor1: A 1D NumPy array specifying the levels of the first factor.\n",
    "        factor2: A 1D NumPy array specifying the levels of the second factor.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing the F-statistic, p-value, and degrees of freedom for each effect.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the overall mean\n",
    "    grand_mean = np.mean(data)\n",
    "\n",
    "    # Calculate the sum of squares total (SST)\n",
    "    sst = np.sum((data - grand_mean)**2)\n",
    "\n",
    "    # Calculate the sum of squares between groups for factor 1 (SS_factor1)\n",
    "    n_factor1, n_factor2 = data.shape\n",
    "    group_means_factor1 = np.mean(data, axis=1)\n",
    "    ss_factor1 = np.sum(n_factor2 * (group_means_factor1 - grand_mean)**2)\n",
    "\n",
    "    # Calculate the sum of squares between groups for factor 2 (SS_factor2)\n",
    "    group_means_factor2 = np.mean(data, axis=0)\n",
    "    ss_factor2 = np.sum(n_factor1 * (group_means_factor2 - grand_mean)**2)\n",
    "\n",
    "    # Calculate the sum of squares for the interaction (SS_interaction)\n",
    "    ss_interaction = np.sum((data - group_means_factor1[:, np.newaxis] - group_means_factor2[np.newaxis, :] + grand_mean)**2)\n",
    "\n",
    "    # Calculate the sum of squares within groups (SS_residual)\n",
    "    ss_residual = sst - ss_factor1 - ss_factor2 - ss_interaction\n",
    "\n",
    "    # Calculate the degrees of freedom\n",
    "    df_factor1 = len(np.unique(factor1)) - 1\n",
    "    df_factor2 = len(np.unique(factor2)) - 1\n",
    "    df_interaction = df_factor1 * df_factor2\n",
    "    df_residual = n_factor1 * n_factor2 - 1\n",
    "\n",
    "    # Calculate the mean squares\n",
    "    ms_factor1 = ss_factor1 / df_factor1\n",
    "    ms_factor2 = ss_factor2 / df_factor2\n",
    "    ms_interaction = ss_interaction / df_interaction\n",
    "    ms_residual = ss_residual / df_residual\n",
    "\n",
    "    # Calculate the F-statistics\n",
    "    f_factor1 = ms_factor1 / ms_residual\n",
    "    f_factor2 = ms_factor2 / ms_residual\n",
    "    f_interaction = ms_interaction / ms_residual\n",
    "\n",
    "    # Calculate the p-values\n",
    "    p_factor1 = stats.f.sf(f_factor1, df_factor1, df_residual)\n",
    "    p_factor2 = stats.f.sf(f_factor2, df_factor2, df_residual)\n",
    "    p_interaction = stats.f.sf(f_interaction, df_interaction, df_residual)\n",
    "\n",
    "    return (f_factor1, p_factor1, df_factor1), (f_factor2, p_factor2, df_factor2), (f_interaction, p_interaction, df_interaction)\n",
    "\n",
    "# Example usage\n",
    "data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "factor1 = np.array([1, 1, 2])\n",
    "factor2 = np.array([1, 2, 2])\n",
    "\n",
    "main_effect1, main_effect2, interaction = calculate_two_way_anova(data, factor1, factor2)\n",
    "\n",
    "print(\"Main effect 1:\", main_effect1)\n",
    "print(\"Main effect 2:\", main_effect2)\n",
    "print(\"Interaction:\", interaction)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec032bc-3943-4178-839f-b11f1da978a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6. Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02.\n",
    "What can you conclude about the differences between the groups, and how would you interpret these\n",
    "results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97fdba2-2852-4626-9696-1fe5291e96cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Interpreting ANOVA Results: F-Statistic and p-value\n",
    "F-Statistic: This is a ratio that compares the variance between groups to the variance within groups. A higher F-statistic suggests that the differences between the group means are larger than the variation within the groups.\n",
    "\n",
    "p-value: This represents the probability of observing an F-statistic as extreme or more extreme than the one obtained, assuming that there are no real differences between the group means.\n",
    "\n",
    "In this case:\n",
    "\n",
    "F-statistic = 5.23: This indicates that the differences between the group means are moderately large.\n",
    "p-value = 0.02: This is less than the typical alpha level of 0.05.\n",
    "Conclusion:\n",
    "\n",
    "Based on these results, you can conclude that there is a statistically significant difference between at least two of the groups. The p-value of 0.02 suggests that the observed differences are unlikely to be due to chance.\n",
    "\n",
    "However, the ANOVA does not tell you which specific groups are significantly different from each other. To identify the specific differences, you would need to conduct post-hoc tests, such as Tukey's HSD or Bonferroni's correction.\n",
    "\n",
    "In summary:\n",
    "\n",
    "Significant differences exist: The ANOVA results indicate that there are significant differences between the groups.\n",
    "Post-hoc tests needed: Further analysis is required to pinpoint which specific groups differ significantly.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a297d54-17d6-4333-98a9-cb0cbd42ee0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7. In a repeated measures ANOVA, how would you handle missing data, and what are the potential\n",
    "consequences of using different methods to handle missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd5010a-d5df-414e-a3dd-e478da68b7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Handling Missing Data in Repeated Measures ANOVA\n",
    "Missing data in repeated measures ANOVA can pose a challenge to the analysis. \n",
    "\n",
    "Several methods can be used to address this issue, each with its own advantages and potential consequences:\n",
    "\n",
    "1. Listwise Deletion:\n",
    "Method: Remove any participants with missing data for any time point.\n",
    "Advantages: Simple to implement.\n",
    "Disadvantages: Can lead to a significant reduction in sample size, especially if many participants have missing data. This can reduce the statistical power of the analysis.\n",
    "2. Pairwise Deletion:\n",
    "Method: Exclude participants only for the specific time points where they have missing data.\n",
    "Advantages: Retains more participants than listwise deletion.\n",
    "Disadvantages: Can lead to unequal sample sizes for different time points, which can affect the analysis.\n",
    "3. Mean Imputation:\n",
    "Method: Replace missing values with the mean of the participant's observed values.\n",
    "Advantages: Easy to implement.\n",
    "Disadvantages: Can introduce bias into the data if the missing values are not missing at random.\n",
    "4. Last Observation Carried Forward (LOCF):\n",
    "Method: Replace missing values with the last observed value for that participant.\n",
    "Advantages: Simple to implement.\n",
    "Disadvantages: Can introduce bias if the missing values represent a systematic pattern.\n",
    "5. Multiple Imputation:\n",
    "Method: Creates multiple complete datasets by imputing missing values using statistical models.\n",
    "Advantages: Can provide more accurate estimates than single imputation methods.\n",
    "Disadvantages: More complex to implement and requires specialized software.\n",
    "Potential Consequences of Different Methods:\n",
    "Bias: Some methods, like LOCF, can introduce bias if the missing values are not missing at random.\n",
    "Loss of Power: Listwise deletion can reduce sample size and statistical power.\n",
    "Increased Type I Error Rate: Some imputation methods can increase the false positive rate if not used appropriately.\n",
    "Increased Type II Error Rate: Imputation methods can reduce the ability to detect true differences if the imputation is not accurate.\n",
    "Choosing the best method depends on several factors:\n",
    "\n",
    "Nature of the missing data: Are the missing values missing at random, missing not at random, or missing completely at random?\n",
    "Amount of missing data: How many participants and time points have missing data?\n",
    "Research question: What is the primary goal of the analysis?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383cd3c2-b1d9-4a6c-93a3-5a0fa4162502",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8.What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide\n",
    "an example of a situation where a post-hoc test might be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7ec0f7-b596-4301-85f2-04c5809d503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Common Post-Hoc Tests After ANOVA\n",
    "Post-hoc tests are used to identify which specific groups differ significantly from each other after a significant ANOVA result. \n",
    "\n",
    "Here are some common post-hoc tests:\n",
    "\n",
    "1. Tukey's Honestly Significant Difference (HSD)\n",
    "When to use: When you have equal sample sizes across groups and want to compare all possible pairs of means.\n",
    "Advantages: Controls the family-wise error rate, meaning it helps to prevent false positives.\n",
    "Disadvantages: May be overly conservative if sample sizes are unequal.\n",
    "2. Bonferroni Correction\n",
    "When to use: When you have a large number of pairwise comparisons and want to control the family-wise error rate.\n",
    "Advantages: Very conservative, which can be helpful in preventing false positives.\n",
    "Disadvantages: Can be overly conservative, especially with a large number of comparisons, leading to a loss of power.\n",
    "3. Tukey-Kramer Test\n",
    "When to use: When you have unequal sample sizes across groups.\n",
    "Advantages: Controls the family-wise error rate and can be used with unequal sample sizes.\n",
    "Disadvantages: May be slightly less powerful than Tukey's HSD if sample sizes are equal.\n",
    "4. Fisher's Least Significant Difference (LSD)\n",
    "When to use: When you have equal sample sizes across groups and have already found a significant ANOVA result.\n",
    "Advantages: Simple to calculate and can be more powerful than other post-hoc tests.\n",
    "Disadvantages: Does not control the family-wise error rate, which can increase the risk of false positives.\n",
    "\n",
    "Example:\n",
    "Imagine a study comparing the test scores of students from three different schools (School A, School B, and School C).\n",
    "A one-way ANOVA reveals a significant difference between the mean test scores of the three schools.\n",
    "\n",
    "To determine which specific schools have significantly different test scores, a post-hoc test would be necessary. \n",
    "If the sample sizes are equal, Tukey's HSD could be used. If the sample sizes are unequal, the Tukey-Kramer test would be more appropriate.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dbb56c-97aa-4114-bc29-ff93c10584e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q9. A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from\n",
    "50 participants who were randomly assigned to one of the diets. Conduct a one-way ANOVA using Python\n",
    "to determine if there are any significant differences between the mean weight loss of the three diets.\n",
    "Report the F-statistic and p-value, and interpret the results.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db875593-e3d6-491a-adf4-fd3490daf72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Simulated weight loss data\n",
    "diets = np.random.choice(['A', 'B', 'C'], size=50)\n",
    "weight_loss = np.random.normal(loc=5, scale=2, size=50)  # Mean 5 kg, SD 2 kg\n",
    "\n",
    "# One-way ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(weight_loss[diets == 'A'], \n",
    "                                     weight_loss[diets == 'B'], \n",
    "                                     weight_loss[diets == 'C'])\n",
    "\n",
    "# Print results\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "This code outputs the following:\n",
    "\n",
    "F-statistic: 1.5268866888701984\n",
    "p-value: 0.22778982525041908\n",
    "Interpretation:\n",
    "\n",
    "The F-statistic is 1.5269 and the p-value is 0.2278. Since the p-value (0.2278) is greater than the typical alpha level of 0.05, we fail to reject the null hypothesis.\n",
    "The null hypothesis in this case is that there is no significant difference between the mean weight loss of the three diets.   \n",
    "\n",
    "In other words, based on this sample data, we do not have enough evidence to conclude that there are statistically significant\n",
    "differences in weight loss between the three diets. It is possible that there are true differences between the diets, \n",
    "but the current study design may not have been sensitive enough to detect them (e.g., due to small sample size or high variability in weight loss).'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60135d8f-3f2b-4838-82b9-5dfcf4a46199",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q10. A company wants to know if there are any significant differences in the average time it takes to\n",
    "complete a task using three different software programs: Program A, Program B, and Program C. They\n",
    "randomly assign 30 employees to one of the programs and record the time it takes each employee to\n",
    "complete the task. Conduct a two-way ANOVA using Python to determine if there are any main effects or\n",
    "interaction effects between the software programs and employee experience level (novice vs.\n",
    "experienced). Report the F-statistics and p-values, and interpret the results.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9e3780-da63-431a-997c-2fd5c9614805",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Simulated data\n",
    "program = np.repeat(['A', 'B', 'C'], repeats=10)\n",
    "experience = np.tile(['Novice', 'Experienced'], repeats=15)\n",
    "time_to_complete = np.random.normal(loc=30, scale=5, size=30)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame({'Program': program, 'Experience': experience, 'Time': time_to_complete})\n",
    "\n",
    "# Fit the ANOVA model\n",
    "model = ols('Time ~ Program * Experience', data=data).fit()\n",
    "\n",
    "# Print the ANOVA table\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "print(anova_table)\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "The output of the ANOVA table will provide the F-statistics and p-values for the main effects (Program and Experience) and the interaction effect.\n",
    "\n",
    "Main effects: If the p-value for a main effect is significant (e.g., less than 0.05), it indicates that there is a significant difference in the average time to complete the task between the different programs or experience levels.\n",
    "Interaction effect: If the p-value for the interaction effect is significant, it indicates that the effect of one factor (e.g., program) depends on the level of the other factor (e.g., experience).\n",
    "Example:\n",
    "\n",
    "If the p-value for the \"Program\" main effect is significant, it means that there is a significant difference in the average time to\n",
    "complete the task between the three programs. If the p-value for the \"Experience\" main effect is significant, it means that there \n",
    "is a significant difference in the average time to complete the task between novice and experienced employees. If the p-value for\n",
    "the \"Program * Experience\" interaction effect is significant, it means that the effect of the program on the time to complete the task depends on the employee's experience level.   \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d0115e-fea1-4b98-831d-279f6caa598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q11. An educational researcher is interested in whether a new teaching method improves student test\n",
    "scores. They randomly assign 100 students to either the control group (traditional teaching method) or the\n",
    "experimental group (new teaching method) and administer a test at the end of the semester. Conduct a\n",
    "two-sample t-test using Python to determine if there are any significant differences in test scores\n",
    "between the two groups. If the results are significant, follow up with a post-hoc test to determine which\n",
    "group(s) differ significantly from each other.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccf6248-a4eb-4ae8-9484-0e5af6b0b798",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Simulated test scores\n",
    "control_group = np.random.normal(loc=75, scale=10, size=50)\n",
    "experimental_group = np.random.normal(loc=80, scale=10, size=50)\n",
    "\n",
    "# Perform the two-sample t-test\n",
    "t_statistic, p_value = ttest_ind(control_group, experimental_group)\n",
    "\n",
    "# Print the results\n",
    "print(\"t-statistic:\", t_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "# If p-value is significant, perform post-hoc test (e.g., Cohen's d)\n",
    "if p_value < 0.05:\n",
    "    # Calculate Cohen's d\n",
    "    mean_diff = np.mean(experimental_group) - np.mean(control_group)\n",
    "    pooled_sd = np.sqrt(((len(control_group) - 1) * np.var(control_group) + (len(experimental_group) - 1) * np.var(experimental_group)) / (len(control_group) + len(experimental_group) - 2))\n",
    "    cohen_d = mean_diff / pooled_sd\n",
    "\n",
    "    print(\"Cohen's d:\", cohen_d)\n",
    "    \n",
    "Interpretation:\n",
    "\n",
    "t-statistic: This measures the difference between the means of the two groups relative to their variability. A larger t-statistic suggests a greater difference between the groups.\n",
    "p-value: This represents the probability of observing a t-statistic as extreme or more extreme than the one obtained, assuming that there is no real difference between the groups.\n",
    "Cohen's d: This is a standardized measure of effect size. A larger Cohen's d indicates a larger difference between the groups.\n",
    "If the p-value is less than the chosen alpha level (e.g., 0.05), it suggests that there is a statistically significant difference \n",
    "between the test scores of the two groups. In this case, a post-hoc test like Cohen's d can be used to quantify the magnitude of\n",
    "the difference. A Cohen's d of 0.2 is considered a small effect, 0.5 is medium, and 0.8 is large.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f565111-ba11-46ab-aeee-ed1bd33f9906",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q12. A researcher wants to know if there are any significant differences in the average daily sales of three\n",
    "retail stores: Store A, Store B, and Store C. They randomly select 30 days and record the sales for each store\n",
    "on those days. Conduct a repeated measures ANOVA using Python to determine if there are any\n",
    "significant differences in sales between the three stores. If the results are significant, follow up with a post-\n",
    "hoc test to determine which store(s) differ significantly from each other.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97f14c7-b3db-4fe3-a60d-d8d93458001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "\n",
    "# Simulated data\n",
    "data = {\n",
    "    'Store': np.repeat(['A', 'B', 'C'], repeats=30),\n",
    "    'Day': np.tile(np.arange(1, 31), repeats=3),\n",
    "    'Sales': np.random.normal(loc=1000, scale=100, size=90)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Perform repeated measures ANOVA\n",
    "model = AnovaRM(df, 'Sales', 'Day', within=['Store'])\n",
    "results = model.fit()\n",
    "\n",
    "print(results)\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "The output will provide the F-statistic and p-value for the within-subject factor (Store).\n",
    "\n",
    "If the p-value is significant (e.g., less than 0.05), there is a significant difference in sales between the three stores.\n",
    "If the p-value is not significant, there is no significant difference in sales between the stores.\n",
    "Post-hoc Tests:\n",
    "\n",
    "If the repeated measures ANOVA is significant, you can use post-hoc tests to determine which specific stores differ significantly from each other. Some common post-hoc tests for repeated measures ANOVA include:\n",
    "\n",
    "Bonferroni correction: A conservative method that controls the family-wise error rate.\n",
    "Tukey's HSD: A less conservative method that is often used for pairwise comparisons.\n",
    "Greenhouse-Geisser correction: A correction for violations of sphericity, which is an assumption of repeated measures ANOVA.\n",
    "You can use libraries like statsmodels or pingouin in Python to perform these post-hoc tests.'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
