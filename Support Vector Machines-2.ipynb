{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "#Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "'''\nRelationship Between Polynomial Functions and Kernel Functions\nIn machine learning algorithms, especially Support Vector Machines (SVMs), polynomial functions and kernel functions are closely related.\n\nPolynomial Kernel:\nA polynomial kernel is a type of kernel function that maps the input data into a higher-dimensional feature space.\nThe mapping is based on a polynomial function of the dot product between the input vectors.\nThe degree of the polynomial determines the complexity of the mapping.\n\nMathematical Form:\nK(x, y) = (x^T * y + c)^d\n\nwhere:\nK(x, y) is the kernel function.\nx and y are the input vectors.\nc is a constant term.\nd is the degree of the polynomial.\n\nRelation to Polynomial Functions:\nThe polynomial kernel effectively transforms the input data into a feature space where the features are polynomials of the original features.\nThe degree of the polynomial determines the complexity of the transformed feature space. A higher degree polynomial can create more complex decision boundaries.\nIn essence, the polynomial kernel is a way to implicitly map the data into a higher-dimensional space using a polynomial function, without explicitly calculating the transformed features. \nThis can be computationally efficient and can help to improve the model's performance by making the data more linearly separable in the transformed space.\n\nKey points:\nPolynomial kernels are a special case of kernel functions.\nThey are used to map data into a higher-dimensional feature space.\nThe degree of the polynomial determines the complexity of the mapping.\nPolynomial kernels can help to improve the performance of machine learning models, especially for non-linearly separable data. '''",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "'''\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\n\n# Load the iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create an SVM classifier with a polynomial kernel\nsvm_poly = SVC(kernel='poly', degree=3)\n\n# Train the model on the training data\nsvm_poly.fit(X_train, y_train)\n\n# Make predictions on the testing data\ny_pred_poly = svm_poly.predict(X_test)\n\n# Evaluate the model's accuracy\naccuracy_poly = accuracy_score(y_test, y_pred_poly)\nprint(\"Accuracy with polynomial kernel:\", accuracy_poly)\n\nIn this example:\n\nWe load the iris dataset.\nWe split the data into training and testing sets.\nWe create an SVM classifier with a polynomial kernel using SVC(kernel='poly', degree=3). The degree parameter controls the degree of the polynomial.\nWe train the model on the training data using fit().\nWe make predictions on the testing data using predict().\nWe evaluate the model's accuracy using accuracy_score(). '''",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "'''\nIncreasing the value of epsilon in Support Vector Regression (SVR) generally leads to a decrease in the number of support vectors.\n\nHere's why:\n\nEpsilon-insensitive loss: SVR uses an epsilon-insensitive loss function, which means that it only considers the errors that exceed epsilon. This allows for some tolerance in the predictions.\nMargin: The larger the epsilon value, the wider the margin around the regression line. This means that more data points can fall within the margin without being considered support vectors.\nFewer support vectors: As more data points fall within the margin, the number of points that define the model (support vectors) decreases.'''",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\naffect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\nand provide examples of when you might want to increase or decrease its value?",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "'''\nImpact of Parameters on SVR Performance\n\nKernel Function\n\nLinear kernel: Suitable for linearly separable data.\nPolynomial kernel: Can handle non-linear relationships between features. The degree of the polynomial determines the complexity of the mapping.\nRadial basis function (RBF) kernel: A popular choice for non-linear relationships. The gamma parameter controls the width of the RBF kernel.\nSigmoid kernel: Less commonly used, but can be effective in certain cases.\n\nC Parameter\n\nRegularization: Controls the trade-off between fitting the training data and minimizing the model's complexity.\nLarger C: Prioritizes fitting the training data, potentially leading to overfitting.\nSmaller C: Prioritizes a simpler model, which can help prevent overfitting but may sacrifice some accuracy.\n\nEpsilon Parameter\n\nTolerance: Determines the width of the epsilon-insensitive tube around the regression line.\nLarger epsilon: Allows for more errors, making the model more tolerant to noise.\nSmaller epsilon: Requires the model to fit the training data more closely, potentially leading to overfitting.\n\nGamma Parameter (RBF kernel only)\n\nWidth: Controls the width of the RBF kernel.\nLarger gamma: Creates a narrower kernel, making the model more sensitive to local variations in the data.\nSmaller gamma: Creates a wider kernel, making the model more tolerant to variations.\n\nExample:\n\nNon-linear data: If the data is not linearly separable, a polynomial or RBF kernel might be more suitable.\nOverfitting: If the model is overfitting, decreasing the C parameter or increasing the epsilon parameter can help.\nUnderfitting: If the model is underfitting, increasing the C parameter or decreasing the epsilon parameter can help.\nSensitivity to noise: If the data is noisy, increasing the epsilon parameter or using a wider RBF kernel can help.\n\nChoosing the right parameters:\n\nGrid search: Explore different combinations of parameters to find the optimal values.\nCross-validation: Evaluate the model's performance on a validation set to avoid overfitting.\nDomain knowledge: Consider the characteristics of your data and the problem domain to guide your parameter choices.'''",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "'''Q5. Assignment:\n* Import the necessary libraries and load the dataseg\n* Split the dataset into training and testing setZ\n* Preprocess the data using any technique of your choice (e.g. scaling, normalizationK\n* Create an instance of the SVC classifier and train it on the training datW\n* Use the trained classifier to predict the labels of the testing datW\n* Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\nprecision, recall, F1-scoreK\n* Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomizedSearchCV to\nimprove its performanc_\n* Train the tuned classifier on the entire dataseg\n* Save the trained classifier to a file for future use.'''",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "'''\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nimport joblib\n\n# Load the dataset (replace 'data.csv' with your actual dataset path)\ndata = pd.read_csv('data.csv')\n\n# Split the data into features and target variable\nX = data.iloc[:, :-1]\ny = data.iloc[:, -1]\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Preprocess the data (e.g., scaling)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Create an instance of the SVC classifier\nsvm = SVC()\n\n# Train the classifier on the training set\nsvm.fit(X_train_scaled, y_train)\n\n# Predict the labels for the testing set\ny_pred = svm.predict(X_test_scaled)\n\n# Evaluate the performance of the classifier\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1-score:\", f1)\n\n# Tune the hyperparameters using GridSearchCV or RandomizedSearchCV\nparam_grid = {\n    'C': [0.1, 1, 10],\n    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n    'gamma': ['scale', 'auto']\n}\n\n# GridSearchCV\n# grid_search = GridSearchCV(svm, param_grid, cv=5)\n# grid_search.fit(X_train_scaled, y_train)\n# best_params = grid_search.best_params_\n\n# RandomizedSearchCV\nrandom_search = RandomizedSearchCV(svm, param_grid, n_iter=10, cv=5)\nrandom_search.fit(X_train_scaled, y_train)\nbest_params = random_search.best_params_\n\n# Train the tuned classifier on the entire dataset\nbest_svm = SVC(**best_params)\nbest_svm.fit(X_train_scaled, y_train)\n\n# Save the trained classifier\njoblib.dump(best_svm, 'svm_model.pkl')        '''",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}