{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "#Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, \nwhile 40% of the employees who use the plan are smokers.\nWhat is the probability that an employee is a smoker given that he/she uses the health insurance plan?",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "'''\nTo calculate the probability of an employee being a smoker given that he/she uses the health insurance plan, we can use conditional probability.\n\nLet:\n\nA = the event that an employee uses the health insurance plan\nB = the event that an employee is a smoker\n\nWe are given:\n\nP(A) = 0.7 (70% of employees use the plan)\nP(B|A) = 0.4 (40% of plan users are smokers)\nWe want to find P(B|A).\n\nUsing Bayes' theorem, we can write:\n\nP(B|A) = P(A|B) * P(B) / P(A)\n\nSince we don't have direct information about P(A|B) or P(B), we can't use this formula directly.\nHowever, we can use the given information to indirectly calculate P(B|A).\nWe know that 40% of plan users are smokers, so P(B|A) = 0.4.\nTherefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.4.   ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "'''\nBernoulli Naive Bayes and Multinomial Naive Bayes are both probabilistic classifiers used in machine learning, but they differ in their underlying assumptions and applications.   \n\nBernoulli Naive Bayes:\n\nAssumption: Each feature is a binary variable (e.g., present or absent).   \nUse Case: Ideal for tasks involving binary features, such as text classification where features represent the presence or absence of specific words.   \nCalculation: Calculates the probability of a class based on the probability of a feature being present or absent in that class.\n\nMultinomial Naive Bayes:\n\nAssumption: Each feature can take on multiple discrete values (e.g., word counts in a document).   \nUse Case: Commonly used for text classification tasks where features represent the frequency of words in a document.   \nCalculation: Calculates the probability of a class based on the frequency of each feature in that class.\n\nKey Differences:\n\nFeature\t              Bernoulli Naive Bayes             \t                             Multinomial Naive Bayes\nFeature Type\t      Binary (0/1)\t                                                     Discrete (counts)\nUse Case\t          Text classification with presence/absence features\t             Text classification with word frequency\nCalculation\t          Based on feature presence/absence                              \t Based on feature counts  '''",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Q3. How does Bernoulli Naive Bayes handle missing values?",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "'''\nBernoulli Naive Bayes handles missing values by treating them as a separate category or value. This means that if a feature value is missing for an instance, it is considered a unique value.\n\nHere's how it works:\n\nCreate a New Category: A new category is added to the feature's vocabulary to represent missing values.\nUpdate Probabilities: The probabilities for this new category are calculated based on the frequency of missing values in the training data.\nClassification: When classifying new instances, the probability of the missing value category is included in the calculations.\n\nExample:\n\nSuppose you have a feature \"has_review\" that is binary (1 if a product has a review, 0 otherwise). \nIf some instances have missing values for this feature, a new category \"missing\" would be added. \nThe classifier would then calculate probabilities like P(has_review=1|class=A), P(has_review=0|class=A), and P(has_review=missing|class=A).\n\nBenefits of this approach:\n\nSimple: It's a straightforward method that doesn't require complex imputation techniques.\nHandles Missingness: It explicitly accounts for missing values, avoiding potential biases.\n\nLimitations:\n\nMight Introduce Bias: If missing values are not missing at random, treating them as a separate category might introduce bias.\nReduced Accuracy: In some cases, treating missing values as a separate category might reduce the classifier's accuracy, especially if the number of missing values is significant. '''",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Q4. Can Gaussian Naive Bayes be used for multi-class classification?",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "'''\nYes, Gaussian Naive Bayes can be used for multi-class classification.\n\nWhile it's often used for binary classification problems, it can be extended to handle multiple classes. \nThe underlying principle remains the same: each class is assumed to be generated from a Gaussian distribution,\nand the probability of a new instance belonging to a class is calculated based on the likelihood of its features under that class's Gaussian distribution.\n\nHere's how it works:\n\nEstimate Parameters: For each class, estimate the mean and variance of each feature.\nCalculate Probabilities: For a new instance, calculate the probability of it belonging to each class.\nPredict Class: Assign the instance to the class with the highest probability.\n\nKey Points:\n\nProbability Calculation: The probability of an instance belonging to a class is calculated using the probability density function of the Gaussian distribution.\nMultiple Classes: The model calculates probabilities for all classes and assigns the instance to the most likely one.\nAssumptions: The Gaussian Naive Bayes assumption of feature independence still applies in multi-class scenarios.    '''",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "'''Q5. Assignment:\nData preparation:\nDownload the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\ndatasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\nis spam or not based on several input features.\nImplementation:\nImplement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\nscikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\ndataset. You should use the default hyperparameters for each classifier.\nResults:\nReport the following performance metrics for each classifier:\nAccuracy\nPrecision\nRecall\nF1 score\nDiscussion:\nDiscuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\nthe case? Are there any limitations of Naive Bayes that you observed?\nConclusion:\nSummarise your findings and provide some suggestions for future work.'''",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "'''not able to answer'''",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}