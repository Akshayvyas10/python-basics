{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "#Q1. What is the mathematical formula for a linear SVM?",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "'''\nThe mathematical formula for a linear Support Vector Machine (SVM) is:\ny = w^T * x + b\n\nWhere:\ny: The predicted class label (-1 or 1).\nw: The weight vector, which determines the orientation of the separating hyperplane.\nx: The input feature vector.\nb: The bias term, which determines the offset of the hyperplane.\nThe goal of the linear SVM is to find the hyperplane that maximizes the margin between the two classes. \n\nThis is achieved by minimizing the following objective function:   \nminimize ||w||^2\nsubject to: y_i * (w^T * x_i + b) >= 1, for all i\n\nWhere:\n\n||w||^2: The squared norm of the weight vector, which is proportional to the inverse of the margin.\ny_i: The true class label for the i-th training example.\nx_i: The input feature vector for the i-th training example.\nThe constraint ensures that all training examples are correctly classified and have a margin of at least 1.'''",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Q2. What is the objective function of a linear SVM?",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "'''\nThe objective function of a linear SVM is:\nminimize ||w||^2\nsubject to: y_i * (w^T * x_i + b) >= 1, for all i\n\nWhere:\n\n||w||^2: The squared norm of the weight vector, which is proportional to the inverse of the margin.\ny_i: The true class label for the i-th training example.\nx_i: The input feature vector for the i-th training example.\nb: The bias term.\nThe objective is to minimize the squared norm of the weight vector while ensuring that all training examples are correctly classified with\na margin of at least 1. This leads to finding the hyperplane that maximizes the margin between the two classes.'''",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Q3. What is the kernel trick in SVM?",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "'''\nThe kernel trick is a mathematical technique used in Support Vector Machines (SVMs) to transform the original input space into \na higher-dimensional feature space. This transformation can help to create a more linearly separable boundary between the classes, \neven if the original data is not linearly separable.\n\nHow it works:\n\nKernel function: A kernel function is chosen to compute the inner product between two data points in the transformed feature space without explicitly calculating the transformed features.\nKernel matrix: A kernel matrix is constructed, where each element represents the inner product between two data points in the transformed feature space.\nSVM optimization: The SVM optimization problem is formulated using the kernel matrix instead of the original input features.\n\nCommonly used kernel functions:\n\nLinear kernel: K(x, y) = x^T * y\nPolynomial kernel: K(x, y) = (x^T * y + c)^d\nRadial basis function (RBF) kernel: K(x, y) = exp(-gamma * ||x - y||^2)\nSigmoid kernel: K(x, y) = tanh(gamma * x^T * y + c)\n\nBenefits of the kernel trick:\n\nNon-linear separability: It allows SVMs to handle non-linearly separable data.\nComputational efficiency: It avoids the explicit calculation of high-dimensional features, which can be computationally expensive.\nFlexibility: A variety of kernel functions can be used to explore different feature spaces.'''",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Q4. What is the role of support vectors in SVM Explain with example",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "'''\nSupport Vectors in SVM\n\nSupport vectors are a subset of training examples that lie on the margin or boundary between the two classes in a Support Vector Machine (SVM).\nThese points play a crucial role in determining the orientation and position of the separating hyperplane.\n\nWhy are they important?\n\nDefine the margin: The distance between the separating hyperplane and the nearest support vectors on either side defines the margin. A larger margin generally indicates better generalization performance.\nDetermine the model: The SVM model is defined solely by the support vectors and their corresponding labels. Non-support vectors have no influence on the model.\nComputational efficiency: The number of support vectors directly affects the computational complexity of the SVM. Fewer support vectors lead to faster training and prediction.\n\nKey points about support vectors:\n\nMargin maximization: SVMs aim to maximize the margin between the classes, which is determined by the support vectors.\nSparsity: SVMs often have a sparse representation, meaning that only a small subset of training examples (support vectors) are needed to define the model.\nSensitivity: Support vectors can be sensitive to outliers or noise in the data, which can affect the model's performance.'''",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in SVM?",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "'''\nSVM: Hyperplane, Marginal Plane, Soft Margin, and Hard Margin\n\nHyperplane:\n\nA hyperplane is a decision boundary that separates the data points into two classes.\nIn a linear SVM, the hyperplane is a linear equation of the form: w^T * x + b = 0.\nIt is oriented perpendicular to the weight vector w.\n\nMarginal Plane:\n\nThe marginal planes are parallel to the hyperplane and define the boundaries of the margin.\nThey are located at a distance of 1/||w|| from the hyperplane.\nThe goal of SVM is to maximize the margin between the marginal planes.\n\nHard Margin:\n\nA hard margin SVM requires all training examples to be correctly classified with a margin of at least 1.\nThis means that there must be no overlap between the classes and the marginal planes.\nHard margin SVMs are sensitive to outliers and may not be suitable for noisy data.\n\nSoft Margin:\n\nA soft margin SVM allows for some misclassifications in order to achieve a larger margin.\nThis is achieved by introducing a slack variable ξ_i for each training example.\nThe objective function is modified to include a penalty term for misclassifications.\n\nVisualization\n\nIn the image:\n\nThe blue and red points represent the two classes.\nThe green line is the hyperplane.\nThe dashed lines are the marginal planes.\nThe points on the marginal planes are the support vectors.\n\nHard Margin:\n\nAll data points are correctly classified and lie on or outside the marginal planes.\nThe margin is maximized.\n\nSoft Margin:\n\nSome data points may be misclassified, but the overall margin is still maximized.\nThe slack variables allow for a trade-off between margin maximization and misclassification.\n\nKey Points:\n\nThe hyperplane determines the decision boundary.\nThe marginal planes define the margin.\nA hard margin SVM requires all data points to be correctly classified.\nA soft margin SVM allows for some misclassifications to achieve a larger margin.\nThe choice between hard and soft margin depends on the data and the desired trade-off between accuracy and robustness. '''",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "'''Q6. SVM Implementation through Iris dataset.\n~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing set.\n~ Train a linear SVM classifier on the training set and predict the labels for the testing set.\n~ Compute the accuracy of the model on the testing set.\n~ Plot the decision boundaries of the trained model using two of the features.\n~ Try different values of the regularisation parameter C and see how it affects the performance of\n  the model.'''",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "'''\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\n\n# Load the iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a linear SVM classifier\nsvm = SVC(kernel='linear')\n\n# Train the model on the training set\nsvm.fit(X_train, y_train)\n\n# Predict the labels for the testing set\ny_pred = svm.predict(X_test)\n\n# Compute the accuracy of the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\n# Visualize the decision boundaries (using the first two features)\nh = 0.02  # Step size in the mesh\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = svm.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\nplt.title('SVM Decision Boundaries')\nplt.show()\n\n# Try different values of the regularization parameter C\nC_values = [0.1, 1, 10]\nfor C in C_values:\n    svm = SVC(kernel='linear', C=C)\n    svm.fit(X_train, y_train)\n    y_pred = svm.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f\"C={C}, Accuracy:\", accuracy)   '''",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}