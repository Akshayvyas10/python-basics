{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d720d384-3116-4c8f-95bc-51c77252fcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd423a8-5be2-4a9a-a479-3d93027e6d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Min-Max Scaling is a normalization technique used in data preprocessing to transform numerical features to a specific range, typically between 0 and 1. It's especially useful when the data has varying scales or when algorithms like neural networks are sensitive to feature magnitudes.\n",
    "\n",
    "How it works:\n",
    "\n",
    "Calculate minimum and maximum values: Determine the minimum and maximum values for each feature.\n",
    "\n",
    "Rescale values: For each data point, apply the following formula:\n",
    "\n",
    "scaled_value = (value - min_value) / (max_value - min_value)\n",
    "\n",
    "This rescales the values to a range between 0 and 1, preserving the relative differences between the original values.\n",
    "\n",
    "Example:\n",
    "\n",
    "Consider a dataset with the following values for the feature \"Age\": 25, 30, 45, 50.\n",
    "\n",
    "Calculate min and max:\n",
    "\n",
    "Min_value = 25\n",
    "Max_value = 50\n",
    "Rescale values:\n",
    "\n",
    "Scaled Age (25) = (25 - 25) / (50 - 25) = 0\n",
    "Scaled Age (30) = (30 - 25) / (50 - 25) = 0.2\n",
    "Scaled Age (45) = (45 - 25) / (50 - 25) = 0.8\n",
    "Scaled Age (50) = (50 - 25) / (50 - 25) = 1\n",
    "When to Use Min-Max Scaling:\n",
    "\n",
    "Feature scaling: When features have different scales (e.g., age in years vs. income in dollars).\n",
    "Algorithm requirements: When algorithms like neural networks are sensitive to feature magnitudes.\n",
    "Preserving relative differences: When you want to preserve the relative differences between values.\n",
    "Advantages of Min-Max Scaling:\n",
    "\n",
    "Simple to implement\n",
    "Preserves original data distribution\n",
    "Interpretable results\n",
    "Disadvantages of Min-Max Scaling:\n",
    "\n",
    "Sensitive to outliers: Outliers can significantly affect the scaled range.\n",
    "May not be ideal for certain algorithms (e.g., K-nearest neighbors) that rely on distance calculations.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dc11e2-252c-4dd6-abcf-ba00007c78bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f50d8ea-93c2-4292-9d94-06d63c15aa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Unit Vector (Normalization) Technique\n",
    "\n",
    "Unit vector normalization, also known as L2 normalization, is another technique used in data preprocessing to scale numerical features. Unlike Min-Max scaling, it doesn't rescale the data to a specific range but instead scales each feature vector to have a unit length (magnitude of 1).\n",
    "\n",
    "How it works:\n",
    "\n",
    "Calculate the Euclidean norm: For each feature vector, compute its Euclidean norm:\n",
    "\n",
    "norm = sqrt(sum(x^2))\n",
    "where x is the feature vector.\n",
    "\n",
    "Divide by the norm: Divide each element of the feature vector by its norm:\n",
    "\n",
    "scaled_feature = x / norm\n",
    "This ensures that the resulting feature vector has a magnitude of 1.\n",
    "\n",
    "Example:\n",
    "\n",
    "Consider a feature vector x = [2, 4, 6].\n",
    "\n",
    "Calculate the norm:\n",
    "\n",
    "norm = sqrt(2^2 + 4^2 + 6^2) = sqrt(56) ≈ 7.48\n",
    "Divide by the norm:\n",
    "\n",
    "scaled_x = [2/7.48, 4/7.48, 6/7.48] ≈ [0.267, 0.534, 0.801]\n",
    "When to Use Unit Vector Normalization:\n",
    "\n",
    "Distance-based algorithms: When using algorithms like K-nearest neighbors or support vector machines that rely on distance calculations.\n",
    "Preserving relative differences: When you want to preserve the relative differences between values while ensuring equal weight for each feature.\n",
    "Advantages of Unit Vector Normalization:\n",
    "\n",
    "Ensures equal weight for features\n",
    "Suitable for distance-based algorithms\n",
    "Robust to outliers\n",
    "Disadvantages of Unit Vector Normalization:\n",
    "\n",
    "May not be ideal for certain algorithms (e.g., linear regression) that rely on the magnitude of features.\n",
    "Comparison with Min-Max Scaling:\n",
    "\n",
    "Scaling range: Min-Max scaling scales to a specific range (0-1), while Unit Vector normalization scales to a unit length.\n",
    "Impact on relative differences: Both preserve relative differences, but Unit Vector normalization ensures equal weight for features.\n",
    "Sensitivity to outliers: Unit Vector normalization is generally less sensitive to outliers than Min-Max scaling.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb655af5-caa2-4ce2-847c-bd48fa1e1e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e34c270-cbab-43cf-9a70-6117b3d32d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Principal Component Analysis (PCA) is a statistical technique used to reduce the dimensionality of a dataset while preserving the most important information. It achieves this by finding a new set of uncorrelated variables (principal components) that capture the maximum variance in the data.   \n",
    "\n",
    "How PCA works:\n",
    "\n",
    "Standardize the data: Ensure that all features have a mean of 0 and a standard deviation of 1.\n",
    "Calculate the covariance matrix: Compute the covariance matrix of the standardized data.\n",
    "Find the eigenvectors and eigenvalues: Determine the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the principal components, and the eigenvalues indicate the variance explained by each component.   \n",
    "Select principal components: Choose the principal components with the highest eigenvalues, as they capture the most variance in the data.\n",
    "Transform the data: Project the original data onto the selected principal components to obtain the reduced-dimensional representation.\n",
    "Example:   \n",
    "\n",
    "Consider a dataset with two features: height and weight. We want to reduce the dimensionality to one.\n",
    "\n",
    "Standardize data: Assume the standardized data is:\n",
    "\n",
    "Height: [-1, 0, 1]\n",
    "Weight: [-0.5, 0, 0.5]\n",
    "Calculate covariance matrix:\n",
    "\n",
    "Covariance matrix = [[1, 0.5], [0.5, 1]]\n",
    "Find eigenvectors and eigenvalues:\n",
    "\n",
    "Eigenvectors: [[0.707, 0.707], [-0.707, 0.707]]\n",
    "Eigenvalues: [1.5, 0.5]\n",
    "Select principal component: The first principal component (with eigenvalue 1.5) explains the most variance.\n",
    "\n",
    "Transform data: Project the original data onto the first principal component:\n",
    "\n",
    "New feature: [0.707 * -1 + 0.707 * -0.5, 0.707 * 0 + 0.707 * 0, 0.707 * 1 + 0.707 * 0.5] = [-1.06, 0, 1.06]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b12791-5b3e-40d0-ac64-a2d164ba9d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9f1f07-bb26-4756-92aa-ef333ac992d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''PCA and Feature Extraction\n",
    "\n",
    "Principal Component Analysis (PCA) is a powerful technique for feature extraction.\n",
    "Feature extraction involves transforming raw data into a new set of features that are more informative,\n",
    "often with a reduced dimensionality.   \n",
    "\n",
    "How PCA is used for Feature Extraction:\n",
    "\n",
    "Dimensionality Reduction: PCA identifies a new set of uncorrelated variables (principal components) that capture the maximum variance in the data. By selecting only the most important principal components, we can effectively reduce the dimensionality of the data.\n",
    "Noise Reduction: PCA can help to remove noise from the data by focusing on the most significant patterns.\n",
    "Feature Engineering: The principal components can be interpreted as new features that are linear combinations of the original features. These new features may be more informative or easier to interpret than the original ones.\n",
    "Example: Image Compression\n",
    "\n",
    "Imagine a large dataset of images. Each image can be represented as a vector of pixel values. Using PCA, we can reduce the dimensionality of these vectors, effectively compressing the images.\n",
    "\n",
    "Create a matrix: Represent each image as a row in a matrix, where each column corresponds to a pixel.\n",
    "Apply PCA: Perform PCA on the matrix to find the principal components.\n",
    "Select principal components: Choose the most important principal components based on their eigenvalues.\n",
    "Project data: Project the original image data onto the selected principal components.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2b5ae7-18ec-4eea-ad80-1015cf33d11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83117102-6b68-4f4d-b6dd-55035b535efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Using Min-Max Scaling for Food Delivery Recommendation System\n",
    "Min-Max scaling is a suitable technique for preprocessing the data in a food delivery recommendation system due to the varying scales of features like price, rating, and delivery time.\n",
    "\n",
    "Steps involved:\n",
    "\n",
    "Identify numerical features: Select the numerical features that need scaling. In this case, price, rating, and delivery time would likely be the relevant features.\n",
    "\n",
    "Determine minimum and maximum values: Calculate the minimum and maximum values for each selected feature.\n",
    "\n",
    "Apply Min-Max scaling: For each data point, use the following formula to rescale the feature values:\n",
    "\n",
    "scaled_value = (value - min_value) / (max_value - min_value)\n",
    "This will transform the values to a range between 0 and 1.\n",
    "\n",
    "Example:\n",
    "\n",
    "Feature                      \tData\n",
    "Price\t                     200, 150, 300\n",
    "Rating\t                     4.5, 3.8, 4.2\n",
    "Delivery Time (minutes)  \t 30, 25, 40\n",
    "\n",
    "Export to Sheets\n",
    "After Min-Max scaling:\n",
    "\n",
    "Feature\t         Scaled Data\n",
    "Price\t          0.2, 0, 1\n",
    "Rating\t          0.875, 0.5, 0.75\n",
    "Delivery Time     0.5, 0.25, 1\n",
    "\n",
    "Export to Sheets\n",
    "Benefits of Min-Max Scaling in this context:\n",
    "\n",
    "Standardization: Ensures that all features are on a comparable scale, preventing features with larger magnitudes from dominating the model.\n",
    "Improved model performance: Many machine learning algorithms, especially those based on distance calculations or neural networks, benefit from standardized data.\n",
    "Interpretability: The scaled values are easier to interpret and compare.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499c7211-205d-41e4-b8eb-8ff2db22c65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc8d68d-50fa-4f7d-9e58-ab2ce863bcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Using PCA for Dimensionality Reduction in Stock Price Prediction\n",
    "Principal Component Analysis (PCA) is a powerful technique for reducing the dimensionality of a dataset while preserving the most important information. In the context of stock price prediction, PCA can be used to identify the most relevant features and simplify the modeling process.\n",
    "\n",
    "Steps involved:\n",
    "\n",
    "Data Preparation:\n",
    "\n",
    "Clean and preprocess the data: Handle missing values, outliers, and inconsistencies.\n",
    "Normalize or standardize features: Ensure all features are on a comparable scale.\n",
    "\n",
    "Feature Selection:\n",
    "\n",
    "Identify relevant features: Based on domain knowledge and exploratory data analysis, select the most likely relevant features for predicting stock prices.\n",
    "\n",
    "Apply PCA:\n",
    "\n",
    "Create a matrix: Represent the selected features as a matrix, where each row corresponds to a data point and each column corresponds to a feature.\n",
    "Compute principal components: Calculate the principal components of the matrix using PCA.\n",
    "Determine explained variance: Calculate the variance explained by each principal component.\n",
    "Select principal components: Choose the principal components that explain a significant portion of the variance in the data. The number of components selected depends on the desired level of dimensionality reduction and the trade-off between accuracy and computational efficiency.\n",
    "\n",
    "Project data:\n",
    "\n",
    "Project the original data onto the selected principal components to obtain the reduced-dimensional representation.\n",
    "\n",
    "Benefits of using PCA in stock price prediction:\n",
    "\n",
    "Reduced dimensionality: PCA can significantly reduce the number of features, simplifying the modeling process and potentially improving computational efficiency.\n",
    "Noise reduction: PCA can help to remove noise from the data, which can improve the accuracy of the prediction model.\n",
    "Feature engineering: The principal components can be interpreted as new features that may be more informative than the original ones.\n",
    "Visualization: PCA can be used to visualize high-dimensional data in a lower-dimensional space, making it easier to understand relationships between features.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b8102b-c842-4fe6-a1e8-b567f174b30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8a1180-f7b4-4249-b61a-80f5b625a477",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Min-Max Scaling to a Range of -1 to 1\n",
    "Given dataset: [1, 5, 10, 15, 20]\n",
    "\n",
    "Steps:\n",
    "\n",
    "Find the minimum and maximum values:\n",
    "\n",
    "Minimum: 1\n",
    "Maximum: 20\n",
    "Calculate the range:\n",
    "\n",
    "Range = Maximum - Minimum = 20 - 1 = 19\n",
    "Apply the Min-Max scaling formula:\n",
    "\n",
    "Scaled value = 2 * ((original value - minimum) / range) - 1\n",
    "Applying the formula to each value:\n",
    "\n",
    "Scaled 1 = 2 * ((1 - 1) / 19) - 1 = -1\n",
    "Scaled 5 = 2 * ((5 - 1) / 19) - 1 ≈ -0.684\n",
    "Scaled 10 = 2 * ((10 - 1) / 19) - 1 ≈ -0.158\n",
    "Scaled 15 = 2 * ((15 - 1) / 19) - 1 ≈ 0.368\n",
    "Scaled 20 = 2 * ((20 - 1) / 19) - 1 = 1\n",
    "Therefore, the scaled values are: [-1, -0.684, -0.158, 0.368, 1].'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc985466-a98d-4101-bf61-8659ba318257",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e734aad8-bd1e-4a84-9cfc-98b093aca8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Performing PCA on a Dataset with Height, Weight, Age, Gender, and Blood Pressure\n",
    "Understanding the Data:\n",
    "\n",
    "The dataset contains a mix of numerical and categorical features. PCA is primarily designed for numerical data. Therefore, we'll need to handle the categorical feature (gender) before applying PCA.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Encode Categorical Features:\n",
    "\n",
    "Convert the categorical feature \"gender\" into numerical representation (e.g., 0 for male, 1 for female).\n",
    "\n",
    "Standardize Numerical Features:\n",
    "\n",
    "Ensure that all numerical features (height, weight, age, and blood pressure) have a mean of 0 and a standard deviation of 1. This is essential for PCA to work effectively.\n",
    "\n",
    "Apply PCA:\n",
    "\n",
    "Create a matrix where each row represents a data point and each column represents a feature.\n",
    "Calculate the covariance matrix of the standardized data.\n",
    "Find the eigenvectors and eigenvalues of the covariance matrix.\n",
    "Sort the eigenvalues in descending order and their corresponding eigenvectors.\n",
    "\n",
    "Determine Explained Variance:\n",
    "\n",
    "Calculate the explained variance ratio for each principal component. This indicates the proportion of variance explained by each component.\n",
    "\n",
    "Choose Number of Principal Components:\n",
    "\n",
    "The number of principal components to retain depends on the desired level of dimensionality reduction and the trade-off between accuracy and computational efficiency.   \n",
    "A common approach is to choose the components that explain a significant portion of the variance (e.g., 95% or 90%).\n",
    "Visualize the cumulative explained variance plot to help make this decision.\n",
    "\n",
    "Example:\n",
    "\n",
    "Assuming you have standardized the data and calculated the principal components, you might observe the following explained variance ratios:\n",
    "\n",
    "Component 1: 60%\n",
    "Component 2: 30%\n",
    "Component 3: 5%\n",
    "Component 4: 3%\n",
    "Component 5: 2%\n",
    "In this case, you might choose to retain the first two principal components, as they explain 90% of the variance in the data. \n",
    "This would reduce the dimensionality from five features to two.\n",
    "\n",
    "Reasons for Choosing Two Components:\n",
    "\n",
    "Significant explained variance: The first two components capture the majority of the information in the data.\n",
    "Computational efficiency: Reducing the dimensionality to two can improve computational efficiency for subsequent modeling tasks.\n",
    "Interpretability: While the interpretation of principal components can be challenging, two components might be more manageable than five.'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
