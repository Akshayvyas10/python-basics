{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9f7602-410a-4181-9e81-23588ac177d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befb7655-4e75-4e4f-85b9-98844c3ec48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Missing Values in a Dataset\n",
    "Missing values are data points that are absent or incomplete in a dataset. They can occur due to various reasons,\n",
    "such as data entry errors, equipment failures, or privacy concerns.\n",
    "\n",
    "Why it's essential to handle missing values:\n",
    "\n",
    "Data quality: Missing values can reduce the quality and reliability of the data.\n",
    "Model performance: Many machine learning algorithms cannot handle missing values directly, leading to inaccurate results.\n",
    "Bias: If missing values are not handled properly, it can introduce bias into the analysis.\n",
    "Algorithms that are not affected by missing values:\n",
    "\n",
    "K-Nearest Neighbors (KNN): KNN can handle missing values by calculating the distance between the missing data point and its nearest neighbors.\n",
    "Decision Trees: Decision trees can handle missing values by creating branches for missing data.\n",
    "Random Forest: As an ensemble of decision trees, Random Forest can naturally handle missing values.\n",
    "Naive Bayes: Naive Bayes can handle missing values by assuming that the missing values are independent of other features.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86f0180-d9c6-4699-921d-8aeeb1bbb771",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2: List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b2eda6-54d6-4118-913a-29b5874983bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Techniques to Handle Missing Data\n",
    "1. Deletion:\n",
    "\n",
    "Listwise Deletion: Remove entire rows or columns containing missing values.\n",
    "Pairwise Deletion: Remove only the data points that are missing for a specific analysis.\n",
    "\n",
    "Example:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample DataFrame with missing values\n",
    "data = {'A': [1, 2, np.nan, 4],\n",
    "        'B': [5, np.nan, 7, 8]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Listwise deletion\n",
    "df_listwise = df.dropna()\n",
    "\n",
    "# Pairwise deletion (within a specific calculation)\n",
    "mean_A = df['A'].mean(skipna=True)\n",
    "\n",
    "2. Imputation:\n",
    "\n",
    "Mean/Median/Mode Imputation: Replace missing values with the mean, median, or mode of the respective column.\n",
    "Hot-deck Imputation: Replace missing values with values from similar data points.\n",
    "Multiple Imputation: Create multiple imputed datasets by filling in missing values with plausible values.\n",
    "\n",
    "Example:\n",
    "\n",
    "# Mean imputation\n",
    "df['A'].fillna(df['A'].mean(), inplace=True)\n",
    "\n",
    "# Hot-deck imputation (assuming 'C' is a similar column)\n",
    "df['A'].fillna(df['C'], inplace=True)\n",
    "\n",
    "# Multiple imputation (using a library like impyute)\n",
    "from impyute.imputers import MultipleImputer\n",
    "\n",
    "imputer = MultipleImputer()\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "\n",
    "3. Interpolation:\n",
    "\n",
    "Linear Interpolation: Interpolate missing values by assuming a linear relationship between adjacent values.\n",
    "Polynomial Interpolation: Interpolate missing values using a polynomial function.\n",
    "\n",
    "Example:\n",
    "\n",
    "# Linear interpolation\n",
    "df['A'].interpolate(method='linear', inplace=True)\n",
    "\n",
    "4. Prediction:\n",
    "\n",
    "Predict missing values: Use a machine learning model to predict missing values based on other features in the dataset.\n",
    "Example:\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "df_imputed = imputer.fit_transform(df)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2a5df1-13c6-444a-8b1e-37ddd1d54942",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50740ad-f040-4f05-827b-a7d0ceff2baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Imbalanced Data\n",
    "Imbalanced data refers to a dataset where the classes are not equally represented. This means that one or more classes have significantly fewer data points than others. For example, in a dataset of credit card transactions, there might be a large number of legitimate transactions but very few fraudulent ones.\n",
    "\n",
    "Consequences of not handling imbalanced data:\n",
    "\n",
    "Biased models: Models trained on imbalanced data can become biased towards the majority class, leading to poor performance on the minority class.\n",
    "Low accuracy: The model may achieve high overall accuracy but perform poorly on the minority class.\n",
    "Misleading evaluation metrics: Traditional metrics like accuracy can be misleading when dealing with imbalanced data.\n",
    "\n",
    "Techniques to handle imbalanced data:\n",
    "\n",
    "Oversampling: Increase the number of samples from the minority class.\n",
    "Random oversampling: Randomly duplicate samples from the minority class.\n",
    "SMOTE (Synthetic Minority Over-sampling Technique): Generate new synthetic samples for the minority class.\n",
    "Undersampling: Reduce the number of samples from the majority class.\n",
    "Random undersampling: Randomly remove samples from the majority class.\n",
    "Class weighting: Assign higher weights to samples from the minority class during training.\n",
    "Ensemble methods: Combine multiple models to improve performance on imbalanced data.\n",
    "Bagging: Create multiple models from bootstrap samples of the data.\n",
    "Boosting: Iteratively train models that focus on misclassified samples.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f4ee5d-0dea-4323-b760-ff36311abe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad00fff-f1cb-473b-9902-8d6ab0cf29e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Up-sampling and Down-sampling\n",
    "Up-sampling and down-sampling are techniques used to adjust the number of data points in a dataset.\n",
    "\n",
    "Up-sampling\n",
    "Definition: Increasing the number of data points in a dataset, typically by duplicating existing samples or creating new synthetic samples.\n",
    "When to use: When the dataset is imbalanced, with a significant disparity in the number of samples between classes. Up-sampling can help balance the classes and improve model performance.\n",
    "Example:\n",
    "In a dataset of credit card transactions, there might be a large number of legitimate transactions and a small number of fraudulent transactions. Up-sampling can be used to duplicate fraudulent transactions to balance the classes, ensuring that the model can learn to detect fraudulent transactions effectively.\n",
    "\n",
    "Down-sampling\n",
    "Definition: Reducing the number of data points in a dataset, typically by randomly removing samples.\n",
    "When to use: When the dataset is too large and computationally expensive to process. Down-sampling can reduce the dataset size without significantly affecting the model's performance.\n",
    "Example:\n",
    "If a dataset contains millions of data points, down-sampling can be used to select a smaller, representative subset of the data for training and testing. This can speed up the training process and reduce computational costs.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc1c74c-4770-4a37-b4b0-496660c4fd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5: What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0019a4-03a4-4503-87d9-178c7c4ed098",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data Augmentation\n",
    "Data augmentation is a technique used to increase the size and diversity of a dataset by creating new samples from existing ones. \n",
    "This can be especially useful when dealing with limited amounts of data or imbalanced datasets.\n",
    "\n",
    "Common data augmentation techniques include:\n",
    "\n",
    "Rotation: Rotating images by random angles.\n",
    "Flipping: Horizontally or vertically flipping images.\n",
    "Scaling: Scaling images to different sizes.\n",
    "Translation: Shifting images horizontally or vertically.\n",
    "Noise addition: Adding random noise to images or other data.\n",
    "Color jittering: Randomly adjusting the color, brightness, or contrast of images.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "\n",
    "SMOTE is a specific oversampling technique used to address class imbalance in datasets. \n",
    "It generates new synthetic samples for the minority class by interpolating between existing minority class samples.\n",
    "\n",
    "How SMOTE works:\n",
    "\n",
    "Identify nearest neighbors: For each minority class sample, find its k nearest neighbors (typically k=5).\n",
    "Create new samples: Generate new synthetic samples along the line segments connecting the minority class sample to its k nearest neighbors.\n",
    "Repeat: Repeat this process for all minority class samples to create a sufficient number of new samples.\n",
    "\n",
    "Advantages of SMOTE:\n",
    "\n",
    "Generates new, realistic samples: SMOTE creates synthetic samples that are similar to the existing minority class samples, helping to improve model performance on the minority class.\n",
    "Doesn't introduce bias: SMOTE avoids the bias that can be introduced by simply duplicating existing samples.\n",
    "\n",
    "Disadvantages of SMOTE:\n",
    "\n",
    "Can introduce noise: Overly aggressive SMOTE can introduce noise into the data, potentially leading to overfitting.\n",
    "May not be suitable for all datasets: SMOTE may not be appropriate for all types of data, especially if the features are highly correlated.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e73c8d2-e823-4284-9ea0-48b23ce37b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6: What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0612cfa3-adf9-4075-aea7-349a2f306f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Outliers in a dataset are data points that significantly deviate from the majority of the data. \n",
    "They can be unusually high or low values that can skew the results of statistical analysis and machine learning models.\n",
    "\n",
    "Why it's essential to handle outliers:\n",
    "\n",
    "Bias: Outliers can introduce bias into the analysis, leading to inaccurate results.\n",
    "Model performance: Outliers can negatively impact the performance of machine learning models, especially those that are sensitive to outliers.\n",
    "Misinterpretation: Outliers can make it difficult to interpret the data and draw meaningful conclusions.\n",
    "\n",
    "Techniques to handle outliers:\n",
    "\n",
    "Deletion: Remove outliers from the dataset.\n",
    "Capping: Replace outliers with a maximum or minimum value.\n",
    "Winsorization: Replace outliers with the nearest non-outlier value.\n",
    "Transformation: Transform the data to reduce the impact of outliers (e.g., log transformation).\n",
    "\n",
    "Robust statistical methods: Use statistical methods that are less sensitive to outliers \n",
    "(e.g., median absolute deviation, robust regression).'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c11da5c-64bd-42cb-800e-f9a527a74b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94df8fd2-3e12-4d34-b958-f57f69069012",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Techniques to Handle Missing Data in Customer Analysis\n",
    "When working with customer data, dealing with missing values is a common challenge.\n",
    "\n",
    "Here are some effective techniques you can use:\n",
    "\n",
    "Deletion Methods:\n",
    "Listwise Deletion: Remove entire rows or columns containing missing values. This is simple but can significantly reduce your dataset.\n",
    "Pairwise Deletion: Remove only the data points that are missing for a specific analysis. This can be more efficient, but it can also introduce bias if missingness is not random.\n",
    "\n",
    "Imputation Methods:\n",
    "Mean/Median/Mode Imputation: Replace missing values with the mean, median, or mode of the respective column. This is a simple approach but can introduce bias if the distribution is skewed.\n",
    "Hot-deck Imputation: Replace missing values with values from similar data points. This can be effective if you have a clear understanding of the relationships between variables.\n",
    "Multiple Imputation: Create multiple imputed datasets by filling in missing values with plausible values. This can provide more accurate estimates and reduce bias.\n",
    "K-Nearest Neighbors (KNN) Imputation: Replace missing values with the average values of the k nearest neighbors. This is a good option if the data is numerical and has a clear distance metric.\n",
    "\n",
    "Prediction Methods:\n",
    "Regression or Machine Learning Models: Use a predictive model to predict missing values based on other variables in the dataset. \n",
    "This can be effective if the missing values are related to other variables.\n",
    "\n",
    "Other Methods:\n",
    "Interpolation: Use interpolation techniques (e.g., linear, polynomial) to fill in missing values in time series data.\n",
    "Creating a Missing Value Indicator: Create a new binary variable indicating whether a value is missing. \n",
    "This can help the model account for missingness.\n",
    "\n",
    "Choosing the Right Technique:\n",
    "\n",
    "The best technique for handling missing data depends on the nature of the data, the amount of missing data, and the goals of your analysis. Consider the following factors when making your decision:\n",
    "\n",
    "Type of data: Numerical or categorical data may require different imputation methods.\n",
    "Amount of missing data: If a large proportion of data is missing, deletion may not be feasible.\n",
    "Impact on analysis: Evaluate how different imputation methods affect your analysis results.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d393fd8-e1ba-447c-a404-4a3425344add",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39798a17-d3d2-4ed7-ac4a-4b390534d1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Determining the Pattern of Missing Data\n",
    "When working with a large dataset containing missing values, it's crucial to understand the pattern of missingness to choose the appropriate handling techniques.\n",
    "\n",
    "Here are some strategies to determine if the missing data is missing at random (MAR) or missing not at random (MNAR):\n",
    "\n",
    "1. Data Visualization:\n",
    "Missingness Patterns: Create visualizations like heatmaps or missing value patterns to identify any patterns in the distribution of missing values.\n",
    "Relationships with Other Variables: Explore if missingness is related to other variables in the dataset. For example, are missing values more common for certain categories or values of other variables?\n",
    "2. Statistical Tests:\n",
    "Chi-square Test: If the missingness is categorical, use a chi-square test to determine if there's a relationship between missingness and other categorical variables.\n",
    "T-test or ANOVA: If the missingness is continuous, use a t-test or ANOVA to compare the means of the missing and non-missing groups for other variables.\n",
    "3. Missingness Indicator:\n",
    "Create a new variable: Create a binary variable indicating whether a value is missing.\n",
    "Analyze relationships: Analyze the relationship between this new variable and other variables in the dataset. If there's a significant relationship, it suggests that missingness is not random.\n",
    "4. Domain Knowledge:\n",
    "Leverage expertise: Use your understanding of the data and the underlying process to identify potential reasons for missingness.\n",
    "Consider causal relationships: Are there any causal relationships between missingness and other variables that might explain the pattern?\n",
    "5. Multiple Imputation Techniques:\n",
    "Sensitivity Analysis: Try different imputation methods and compare the results. If the results are significantly different, it might indicate that missingness is not random.\n",
    "\n",
    "Types of Missingness:\n",
    "\n",
    "Missing Completely at Random (MCAR): Missingness is unrelated to any other variables in the dataset.\n",
    "Missing at Random (MAR): Missingness is related to other observed variables in the dataset, but not to the missing values themselves.\n",
    "Missing Not at Random (MNAR): Missingness is related to the missing values themselves, indicating a systematic pattern.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c05fc6-4518-4cc0-9156-58f249d1f4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cd2801-df5a-4f65-93a5-8bcbb3ecfb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Evaluating Models on Imbalanced Datasets\n",
    "When working with imbalanced datasets, it's essential to use evaluation metrics that are not heavily skewed by the majority class.\n",
    "\n",
    "Here are some effective strategies:\n",
    "\n",
    "1. Precision, Recall, and F1-score:\n",
    "Precision: Measures the proportion of positive predictions that are actually correct.\n",
    "Recall: Measures the proportion of positive instances that are correctly predicted.\n",
    "F1-score: The harmonic mean of precision and recall, providing a balanced metric.\n",
    "2. Confusion Matrix:\n",
    "A table that summarizes the performance of a classification model.\n",
    "It can help visualize the number of true positives, true negatives, false positives, and false negatives.\n",
    "3. ROC Curve and AUC:\n",
    "ROC curve: Plots the true positive rate against the false positive rate.\n",
    "AUC: Area under the ROC curve, a measure of the model's overall performance.\n",
    "4. F-beta score:\n",
    "A weighted harmonic mean of precision and recall, allowing you to prioritize either precision or recall.\n",
    "5. Sensitivity and Specificity:\n",
    "Sensitivity: Measures the proportion of positive instances that are correctly predicted (recall).\n",
    "Specificity: Measures the proportion of negative instances that are correctly predicted.\n",
    "6. Cost-sensitive learning:\n",
    "Assign different costs to misclassifications based on the consequences of each error.\n",
    "This can help to address the imbalance in the dataset.\n",
    "7. Oversampling and Undersampling:\n",
    "Oversampling: Increase the number of samples from the minority class.\n",
    "Undersampling: Decrease the number of samples from the majority class.\n",
    "8. SMOTE (Synthetic Minority Over-sampling Technique):\n",
    "Generate new synthetic samples for the minority class to balance the dataset.\n",
    "9. Class weighting:\n",
    "Assign higher weights to samples from the minority class during training.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87d43f7-667c-4cee-81e7-3b1112a44188",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4510de54-c426-4459-ad51-951c04ecf2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Balancing Imbalanced Datasets\n",
    "When working with imbalanced datasets, where one class (in this case, satisfied customers) significantly outnumbers the other, it's crucial to employ techniques to balance the classes and avoid biased models. Here are some methods you can use to down-sample the majority class:\n",
    "\n",
    "Random Undersampling:\n",
    "Simple approach: Randomly select a subset of samples from the majority class to match the number of samples in the minority class.\n",
    "Cluster-Based Undersampling:\n",
    "Group similar samples: Cluster the majority class samples and randomly select one sample from each cluster. This helps preserve diversity within the majority class.\n",
    "\n",
    "Tomek Links:\n",
    "Identify pairs: Identify pairs of samples from different classes that are nearest neighbors to each other.\n",
    "Remove majority class samples: Remove the majority class sample from each Tomek link pair.\n",
    "Edited Nearest Neighbors (ENN):\n",
    "Identify misclassified samples: Identify majority class samples that are misclassified by a classifier trained on the balanced dataset.\n",
    "Remove misclassified samples: Remove these misclassified samples from the majority class.\n",
    "Hybrid Techniques:\n",
    "Combine multiple down-sampling techniques to achieve a balance between bias and variance.\n",
    "\n",
    "Example using Python:\n",
    "\n",
    "import pandas as pd\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, EditedNearestNeighbors\n",
    "\n",
    "# Assuming 'customer_data' is your DataFrame\n",
    "X = customer_data.drop('satisfaction', axis=1)\n",
    "y = customer_data['satisfaction']\n",
    "\n",
    "# Random undersampling\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "\n",
    "# Cluster-based undersampling (using KMeans clustering)\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "X_clustered = kmeans.fit_transform(X)\n",
    "rus = RandomUnderSampler(random_state=42, sampling_strategy='auto')\n",
    "X_resampled, y_resampled = rus.fit_resample(X_clustered, y)\n",
    "\n",
    "# Tomek links\n",
    "tl = TomekLinks()\n",
    "X_resampled, y_resampled = tl.fit_resample(X, y)\n",
    "\n",
    "# Edited nearest neighbors\n",
    "enn = EditedNearestNeighbors()\n",
    "X_resampled, y_resampled = enn.fit_resample(X, y)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19245fd-3686-406a-85ce-a47252ade890",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6f8f64-70fc-46c2-88b1-53fd27c880f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Balancing Imbalanced Datasets: Up-sampling Minority Class\n",
    "When working with imbalanced datasets, where one class (the minority class) is significantly underrepresented, it's crucial to employ techniques to balance the classes and avoid biased models.\n",
    "\n",
    "Here are some methods you can use to up-sample the minority class:\n",
    "\n",
    "Random Over-sampling:\n",
    "Simple approach: Randomly duplicate samples from the minority class to match the number of samples in the majority class.\n",
    "Synthetic Minority Over-sampling Technique (SMOTE):\n",
    "Create new samples: Generate new synthetic samples for the minority class by interpolating between existing minority class samples.\n",
    "Adaptive Synthetic Sampling (ADASYN):\n",
    "Focus on difficult samples: Focus on generating new samples in regions where the minority class is underrepresented.\n",
    "Borderline-SMOTE:\n",
    "Identify borderline samples: Identify minority class samples that are near the decision boundary.\n",
    "Generate new samples: Generate new samples from these borderline regions.\n",
    "K-Means-SMOTE:\n",
    "Cluster minority class: Cluster the minority class samples and generate new samples within each cluster.\n",
    "\n",
    "Example using Python:\n",
    "\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "X = data.drop('target_variable', axis=1)\n",
    "y = data['target_variable']\n",
    "\n",
    "# Random oversampling\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "# SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# ADASYN\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_resampled, y_resampled = adasyn.fit_resample(X, y)'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
