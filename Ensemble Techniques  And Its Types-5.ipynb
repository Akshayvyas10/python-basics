{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "'''Q1. You are working on a machine learning project where you have a dataset containing numerical and\ncategorical features. You have identified that some of the features are highly correlated and there are\nmissing values in some of the columns. You want to build a pipeline that automates the feature\nengineering process and handles the missing values.\nDesign a pipeline that includes the following steps\"\nUse an automated feature selection method to identify the important features in the datasetC\nCreate a numerical pipeline that includes the following steps\"\nImpute the missing values in the numerical columns using the mean of the column valuesC\nScale the numerical columns using standardisationC\nCreate a categorical pipeline that includes the following steps\"\nImpute the missing values in the categorical columns using the most frequent value of the columnC\nOne-hot encode the categorical columnsC\nCombine the numerical and categorical pipelines using a ColumnTransformerC\nUse a Random Forest Classifier to build the final modelC\nEvaluate the accuracy of the model on the test dataset.\n\nNote: Your solution should include code snippets for each step of the pipeline, and a brief explanation of\neach step. You should also provide an interpretation of the results and suggest possible improvements for\nthe pipeline.'''",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "'''\nPipeline Steps\n1. Import Necessary Libraries\n\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\n2. Load Dataset\n\n# Replace 'your_dataset.csv' with your actual dataset path\ndata = pd.read_csv('your_dataset.csv')\n\n3. Split Data into Features and Target\n\nX = data.drop('target_column', axis=1)  # Replace 'target_column' with your target variable\ny = data['target_column']\n\n4. Split Data into Training and Testing Sets\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n5. Create Numerical and Categorical Pipelines\n\nnumerical_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\ncategorical_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n6. Combine Pipelines Using ColumnTransformer\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_pipeline, numerical_features),\n        ('cat', categorical_pipeline, categorical_features)\n    ])\n\n7. Feature Selection\n\nselector = SelectFromModel(RandomForestClassifier(random_state=42))\n\n8. Create Final Pipeline\n\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('feature_selection', selector),\n    ('classifier', RandomForestClassifier(random_state=42))\n])\n\n9. Train the Model\n\npipeline.fit(X_train, y_train)\n\n10. Evaluate the Model\n\naccuracy = pipeline.score(X_test, y_test)\nprint(\"Accuracy:\", accuracy)\n\n# Cross-validation for more robust evaluation\ncv_scores = cross_val_score(pipeline, X, y, cv=5)\nprint(\"Cross-Validation Accuracy:\", cv_scores.mean())\n\nExplanation of Steps:\nPreprocessing: The pipelines handle missing values and scale numerical features while one-hot encoding categorical features.\nFeature Selection: The SelectFromModel step uses a Random Forest Classifier to identify important features based on their feature importance.\nPipeline Combination: The ColumnTransformer combines the numerical and categorical pipelines for consistent preprocessing.\nModel Training: The Random Forest Classifier is trained on the preprocessed and selected features.\nEvaluation: The model's accuracy is evaluated on the test set and using cross-validation for more robust assessment.'''",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Q2. Build a pipeline that includes a random forest classifier and a logistic regression classifier, and then\nuse a voting classifier to combine their predictions. Train the pipeline on the iris dataset and evaluate its\naccuracy.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "'''\nCreating a Pipeline with Random Forest and Logistic Regression\n\nImport necessary libraries:\n\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\nLoad the Iris dataset:\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nSplit data into training and testing sets:\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nCreate individual classifiers: Â  \n\nrf = RandomForestClassifier(random_state=42)\nlr = LogisticRegression(random_state=42)\n\nCreate a voting classifier:\n\nvoting_clf = VotingClassifier(estimators=[('rf', rf), ('lr', lr)], voting='hard')\n\nTrain the pipeline:\n\nvoting_clf.fit(X_train, y_train)\n\nEvaluate the pipeline:\n\naccuracy = voting_clf.score(X_test, y_test)\nprint(\"Accuracy:\", accuracy)\n\ncv_scores = cross_val_score(voting_clf, X, y, cv=5)\nprint(\"Cross-Validation Accuracy:\", cv_scores.mean())\n\nExplanation:\n\nWe import the necessary libraries for creating a pipeline with Random Forest and Logistic Regression classifiers, and for using a VotingClassifier.\nWe load the Iris dataset and split it into training and testing sets.\nWe create individual Random Forest and Logistic Regression classifiers.\nWe create a VotingClassifier that combines the predictions of the two classifiers using a 'hard' voting strategy (majority vote).\nWe train the VotingClassifier on the training data.\nWe evaluate the accuracy of the VotingClassifier on the testing data and using cross-validation for a more robust evaluation.\nThis pipeline demonstrates how to combine multiple classifiers using a VotingClassifier and evaluate its performance on a given dataset.  '''",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}