{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "#Q1. What is Gradient Boosting Regression?",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "'''\nGradient Boosting Regression is a machine learning algorithm that belongs to the ensemble family. \nIt iteratively trains a series of weak models (often decision trees) and combines their predictions to create a strong predictive model.\nUnlike Random Forest, which creates models independently, Gradient Boosting focuses on improving the performance of the ensemble by adjusting the weights of training instances based on their classification accuracy by previous models.\n\nKey components of Gradient Boosting Regression:\n\nWeak Models: Typically decision trees.\nIterative Training: Models are trained sequentially, with each model focusing on the errors made by previous models.\nGradient Descent: The algorithm uses gradient descent to minimize the loss function, optimizing the weights of the individual models.\nEnsemble: The final prediction is a weighted sum of the predictions from all the weak models.\n\nHow it works:\n\nInitialize: A weak model is trained on the entire training dataset.\nCalculate Residuals: The residuals (differences between actual and predicted values) are calculated.\nTrain New Model: A new weak model is trained to predict the residuals.\nUpdate Predictions: The predictions of the new model are scaled and added to the predictions of previous models.\nRepeat: Steps 2-4 are repeated for a specified number of iterations.\n\nAdvantages of Gradient Boosting Regression:\n\nAccurate: Often achieves high accuracy, especially when the base models are weak learners.\nHandles Complex Patterns: Can handle complex relationships in the data.\nFlexible: Can be applied to various tasks, including regression, classification, and ranking.\nInterpretable: The importance of features can be assessed based on the contribution of each weak model.\n\nDisadvantages of Gradient Boosting Regression:\n\nComputational Cost: Can be computationally expensive, especially for large datasets or deep models.\nOverfitting: Prone to overfitting if not carefully tuned.\nSensitive to Hyperparameters: The performance is sensitive to the choice of hyperparameters.\n\nGradient Boosting Regression is a powerful and versatile algorithm that can be used for various regression tasks.\nIts ability to handle complex patterns and achieve high accuracy makes it a popular choice in machine learning.  '''",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\nsimple regression problem as an example and train the model on a small dataset. Evaluate the model's\nperformance using metrics such as mean squared error and R-squared.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "'''import numpy as np\n\ndef gradient_boosting_regression(X, y, n_estimators=100, learning_rate=0.1):\n    \"\"\"\n    Implements a simple gradient boosting regression algorithm.\n\n    Args:\n        X: Input features\n        y: Target variable\n        n_estimators: Number of boosting iterations\n        learning_rate: Learning rate for updating predictions\n\n    Returns:\n        The trained gradient boosting model\n    \"\"\"\n\n    m, n = X.shape\n    y_pred = np.zeros(m)\n    models = []\n\n    for i in range(n_estimators):\n        residuals = y - y_pred\n        model = DecisionTreeRegressor(max_depth=1)  # Simple decision tree\n        model.fit(X, residuals)\n        models.append(model)\n        y_pred += learning_rate * model.predict(X)\n\n    return models\n\n# Example usage\nX = np.array([[1], [2], [3], [4], [5]])\ny = np.array([2, 4, 5, 4, 5])\n\nmodel = gradient_boosting_regression(X, y)\n\n# Make predictions\ny_pred = np.sum([model.predict(X) for model in model], axis=0)\n\n# Evaluate performance\nmse = np.mean((y - y_pred) ** 2)\nr2 = 1 - np.sum((y - y_pred) ** 2) / np.sum((y - np.mean(y)) ** 2)\n\nprint(\"Mean Squared Error:\", mse)\nprint(\"R-squared:\", r2) '''",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\noptimise the performance of the model. Use grid search or random search to find the best\nhyperparameters",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "'''Hyperparameter Tuning for Gradient Boosting Regression\nUnderstanding the Problem:\nWe want to optimize the performance of our Gradient Boosting Regression model by tuning its hyperparameters. \nWe'll use grid search to explore different combinations of hyperparameters and find the best configuration.\n\nHyperparameters to Tune:\n\nn_estimators: Number of boosting stages\nlearning_rate: Learning rate for updates\nmax_depth: Maximum depth of individual trees\n\nUsing GridSearchCV:\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Create a gradient boosting regressor\ngbm = GradientBoostingRegressor()\n\n# Define the hyperparameter grid\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'learning_rate': [0.01, 0.1, 0.3],\n    'max_depth': [3, 4, 5]\n}\n\n# Create a grid search object\ngrid_search = GridSearchCV(gbm, param_grid, cv=5, scoring='neg_mean_squared_error')\n\n# Fit the grid search to the data\ngrid_search.fit(X_train, y_train)\n\n# Best parameters and score\nbest_params = grid_search.best_params_\nbest_score = -grid_search.best_score_\n\nprint(\"Best Parameters:\", best_params)\nprint(\"Best Score (Mean Squared Error):\", best_score)\n\n'''",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Q4. What is a weak learner in Gradient Boosting?",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "'''\nIn Gradient Boosting, a weak learner is a simple model that is individually not very accurate but can still make reasonable predictions.\nThese models are often used as base learners in the boosting process.\n\nCommon examples of weak learners in Gradient Boosting include:\n\nDecision Stumps: These are decision trees with a maximum depth of 1, meaning they make a single split on a single feature.\nShallow Decision Trees: While not as simple as decision stumps, shallow decision trees with limited depth are still considered weak learners compared to deep trees.\nLinear Models: Simple linear models like linear regression or logistic regression can also be used as weak learners.\n\nThe key idea behind using weak learners in Gradient Boosting is that their combined predictions can be more accurate than the predictions of a \nsingle, more complex model. By iteratively training weak models and focusing on the errors made by previous models, \nGradient Boosting can create a strong ensemble that effectively captures complex patterns in the data.            '''",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Q5. What is the intuition behind the Gradient Boosting algorithm?",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "'''The intuition behind Gradient Boosting is to iteratively train weak models and focus on the errors made by previous models.\n\nHere's a breakdown of the underlying concepts:\n\nResiduals: Gradient Boosting starts by training a base model on the original data. The residuals, which are the differences between the actual and predicted values, are then calculated.\nFocus on Errors: In subsequent iterations, new models are trained to predict these residuals. This means that the algorithm is essentially focusing on the parts of the data that the previous models struggled to predict accurately.\nEnsemble: The predictions of all the weak models are combined, with the weights of each model determined based on its contribution to reducing the overall error.\n\nKey ideas:\n\nIterative Improvement: By repeatedly focusing on the errors of previous models, Gradient Boosting can gradually improve the overall accuracy.\nEnsemble: The final prediction is an ensemble of the predictions from all the weak models, leveraging the collective wisdom of the ensemble.\nGradient Descent: The algorithm uses gradient descent to optimize the weights of the models, ensuring that each new model contributes to reducing the overall error. '''",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "'''Gradient Boosting builds an ensemble of weak learners by iteratively training new models to predict the residuals from previous models.\n\nHere's a breakdown of the process:\n\nInitialize: A base model (often a decision tree) is trained on the original data.\nCalculate Residuals: The residuals, which are the differences between the actual and predicted values, are calculated.\nTrain New Model: A new weak model is trained to predict these residuals.\nUpdate Predictions: The predictions of the new model are scaled and added to the predictions of previous models.\nRepeat: Steps 2-4 are repeated for a specified number of iterations.\n\nKey points:\n\nIterative Process: Gradient Boosting is an iterative process where each new model focuses on the errors made by previous models.\nResiduals: The algorithm focuses on predicting the residuals, which represent the parts of the data that the previous models struggled to capture.\nEnsemble: The final prediction is an ensemble of the predictions from all the weak models, weighted according to their contribution to reducing the overall error. '''",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "'''Constructing the Mathematical Intuition of Gradient Boosting\n\nGradient Boosting is a powerful ensemble learning technique that iteratively trains weak models to create a strong predictive model. To understand its mathematical intuition, we can break it down into the following steps:\n\nLoss Function: The first step is to define a loss function that quantifies the error between the predicted and actual values. Common loss functions include squared error for regression and log loss for classification.\nInitialization: A base model (often a decision tree) is trained on the original data.\nResidual Calculation: The residuals, which are the differences between the actual and predicted values, are calculated.\nGradient Calculation: The gradient of the loss function with respect to the predictions is computed. This gradient indicates the direction in which the predictions should be adjusted to minimize the loss.\nUpdate Predictions: The predictions are updated by adding a scaled version of the gradient to the previous predictions. The scaling factor, known as the learning rate, controls the step size in the optimization process.\nIterative Process: Steps 3-5 are repeated for a specified number of iterations, with each iteration focusing on reducing the loss.\n\nMathematical Formulation:\n\nLet:\n\ny: The true target variable\ny_pred: The predicted values\nL: The loss function\nh_m: The m-th weak model\nα_m: The learning rate for the m-th model\n\nThe gradient boosting algorithm can be expressed as follows:\n\nInitialize: y_pred = h_0(X)\nFor m = 1 to M:\nCalculate residuals: r_m = -∂L(y, y_pred) / ∂y_pred\nTrain weak model: h_m = fit_model(X, r_m)\nUpdate predictions: y_pred = y_pred + α_m * h_m(X)   '''",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}